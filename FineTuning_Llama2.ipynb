{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMViLg6auhmiZlpJLRYXuT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*   !pip install : used as package installer for python\n",
        "*   -q : its \"quiet\" mode that means it will shows less output during installation proc\n",
        "*   accelerate == 0.21.0 : optimizes and accelerates training of deep learning models\n",
        "*   peft == 0.4.0 : for efficient fine tuning of LLMs\n",
        "*   bitsandbytes -- 0.4.0 : provides optimized numerical operations for deep learning\n",
        "*   tranformers == 4.31.0 : by hugging face that has pre trained models for NLP tasks\n",
        "*   trl == 0.4.7 : stands for transformer reinforcement learning and used for fine tuning LLM using reinforcement learning techniques\n",
        "*   ~= operator is compatible release to install latest pip version and compatible w/ specified versions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JQLJlKYgMy8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate~=0.21.0 peft~=0.4.0 bitsandbytes~=0.40.2 transformers~=4.31.0 trl~=0.4.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tYeGbziM3ZU",
        "outputId": "5a82bad7-1ee4-44c2-e09b-f43648fb0ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/244.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   import os : functions to interact w/ OS i.e. python script can talk to the sys\n",
        "*   import torch : pytorch by facebook. help build and train neural networks\n",
        "*   from datasets import load_dataset : by hugging face and used to easily download and prepare datasets to train and test\n",
        "*   AutoModelForCausalLM : class that automatically loads pre-trained language models that trained on text dataset to generate coherent natural sounding language\n",
        "*   AutoTokenizer : represents tokenizer that converts text into format that model can understand and smaller parts called tokens.\n",
        "*   BitsAndBytesConfig : for efficient compression and decompression of data for model storage and inference.\n",
        "*   HfArgumentParser : parser for command-line arguments specifically designed to work with Hugging face ecosystem and allows users to pass options to script when running from cmd\n",
        "*   TrainingArguments : contains arguments related to training process like learning rate, batch size, no. of epochs, etc\n",
        "*   pipeline : chains multiple NLP tasks like text generation/classification, etc to process data in pipeline fashion\n",
        "*   logging : this logs info, warnings and errors during model training, debugging and evaluation process\n",
        "*   LoraConfig : stands for Low-Rank Adaptation that helps fine-tune pretrained models w/ fewer parameters and help them adapt to new tasks and datasets\n",
        "*   PeftModel : Parameter-Efficient Fine-Tuning is type of AI model to be efficient and adaptable\n",
        "*   SFTTrainer : class used for supervised fine-tuning (SFT) of lang models and simplifies process of training and tuning models on specific tasks or datasets\n"
      ],
      "metadata": {
        "id": "N-6NhQobTDYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "uecwK2ddTCei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   NousResearch/Llama-2-7b-chat-hf : the chatbot model that we will be using.\n",
        "*   mlabonne/guanaco-llama2-1k : this is the dataset that we will use to train and test the model\n",
        "*   llama-2-7b-miniguanaco : the new name you are going to give to the fine tuned model\n",
        "\n",
        "> syntax for using hugging face:\n",
        "*   username/model_name\n",
        "*   author/dataset_name\n",
        "*   we find out the model card of the model you want to use and then write it in this syntax to use it\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oitOYAv_tFFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-2-7b-miniguanaco\""
      ],
      "metadata": {
        "id": "tb_a1b1etFoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QLoRA parameters\n",
        "*   lora_r = 64 : sets rank of low-rank matrices used. higher rank generally means more expressiveness but more computational cost. 64 is standard. it helps the model reduce the complexity of understanding the data. it basically determines the size\n",
        "*   lora_alpha = 16 : this determines how strongly should it influence. it scales the output of LoRA layers before they are combined with w/ models original output. it balances contribution of og model and the changes by LoRA\n",
        "*   lora_dropout = 0.1 : probability of randomly setting neuron's output to zero during training. it helps model prevent model from memorizing training data amd encourages it to learn more robust patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "FB2To3kM3HW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1"
      ],
      "metadata": {
        "id": "ewW9IsOe3H7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### bitsandbytes parameters\n",
        "*   use_4bit = True : choose a specific small bit of data to load so its faster and easier rather than using any large amount. compressing the brain of the model but not reducing its intelligence\n",
        "*   bnb_4bit_compute_dtype = \"float16\" : need the 4 bit computation to be calculated in only float 16 data type. creating balance between speed and precision\n",
        "*   bnb_4bit_quant_type = \"nf4\" : setting quantization to be performed using 4 bit integers. this quantization works best for models like Llama 2 cause doesnt distort its knowledge too much\n",
        "*   use_nested_quant = False : nested quantization means the model will be made more smaller and concise to be accurate although we are not doing it. if we quantize more then it could damage the performance.\n",
        "*   amp_enabled = True : allows you to perform computations in a lower precision (e.g., FP16) while maintaining FP32 precision for critical parts of the model. This can help reduce memory usage and improve training speed.\n",
        "*   gradient_accumulation_steps = 2 : If memory is a constraint, use gradient accumulation to effectively increase the batch size without requiring more GPU memory. This can improve the stability and efficiency of fine-tuning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FYKtX4iAFOTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "# Enable AMP (Automatic Mixed Precision)\n",
        "amp_enabled = True\n",
        "\n",
        "# Gradient accumulation to increase effective batch size\n",
        "gradient_accumulation_steps = 2"
      ],
      "metadata": {
        "id": "7IgBWv6XFO2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TrainingArguments parameters\n",
        "*   output_dir = \"./results\" : dir where model's predictions and states will be stored\n",
        "*   num_train_epochs = 4 : no. of times entire training data will be passed thru the model\n",
        "*   fp16 = True, bf16 = False : they control whether use 16 or 32 bit floating pt precision for computations\n",
        "*   per_device_train_batch_size = 8, per_devce_eval_batch_size = 8 : set bactch size per GPU for training and evaluating and take 8 eg at a time\n",
        "*   gradient_accumulation_steps = 1 : number of update steps to accumulate gradients before performing a single optimization steps. Setting it to 1 means the gradients are not accumulated and are used immediately for optimization.\n",
        "*   gradient_checkpointing = True : we enable gradient checkpointing. technique to reduce memory storage during training and keep checking if anything goes wrong or not\n",
        "*   max_grad_norm = 0.3 : Gradient clipping is applied to prevent the gradients from becoming too large, which can cause instability in training\n",
        "*   learning_rate = 2e-4 : sets the initial learning rate for the AdamW optimizer to 0.0002. The learning rate determines the step size at which the model's parameters are updated during training.\n",
        "*   weight_decay = 0.001 : Weight decay is a regularization technique that adds a penalty term to the loss function, encouraging the model to have smaller parameter values and prevent overfitting.\n",
        "*   optim = \"paged_adamw_32bit\" : variant of the AdamW optimizer that uses 32-bit precision and paged memory for efficient memory usage.\n",
        "*   lr_scheduler_type = \"cosine\" : learning rate scheduler determines how the learning rate changes over the course of training. A cosine scheduler gradually decreases the learning rate following a cosine curve.\n",
        "*   max_steps = -1 : If set to -1, the number of training steps will be determined based on the num_train_epochs and the size of the training dataset.\n",
        "*   warmup_ratio = 0.03 : learning rate gradually increases from 0 to the initial learning rate over a specified fraction of the total training steps. In this case, 3% of the training steps will be used for warmup.\n",
        "*   group_by_length = True : enables grouping sequences into batches with the same length. This can save memory and speed up training by avoiding padding and allowing more efficient computation.\n",
        "*   save_steps = 0 : sets the number of update steps after which the model checkpoint is saved. If set to 0, the model will not be saved during training.\n",
        "*   logging_steps = 25 : sets the logging frequency. The training progress and metrics will be logged every 25 update steps.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X2ZJMQYBSyI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 4\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = True\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 8\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 8\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 500\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 50\n"
      ],
      "metadata": {
        "id": "U4qvCGh3SyqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SFT parameters\n",
        "*   max_seq_length = None : it means that the model will use the maximum sequence length that it was pretrained with. Some models have a fixed limit on how much text they can process at once. Setting this to None means we're not imposing any such limit.\n",
        "*   packing = False : Sequence packing is a technique where multiple short examples are packed into the same input sequence to increase efficiency.it means that each example will be processed independently, without any packing.\n",
        "*   device_map = {\"\":0} : device mapping determines which parts of the model are loaded on which devices (GPUs). this means that the entire model will be loaded on GPU 0. empty string \"\" represents the entire model, and 0 represents the GPU index.\n",
        "*   List item\n"
      ],
      "metadata": {
        "id": "n0t4-TLrsTJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = True\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "Il5u_qXSsTsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   dataset = load_dataset(dataset_name, split = \"train\") : this is to load the dataset that we had initialized above. the dataset is then split into training dataset.\n",
        "*   compute_dtype = getattr(torch, bnb_4bit_compute_dtype) : we're like choosing the right tools for the job. so here we choose the torch library that works perfectly for numbers and then we pass the datatype that defined above.\n",
        "*   bnb_config = BitsAndBytesConfig(...) : define the QLoRA as we did above\n",
        "*   load_in_4bit = use_4bit : shrinking the size of information so its easier for model to handle and it also saves space and processes information faster\n",
        "*   bnb_4bit_quant_type = bnb_4bit_quant_type : here we define choosing the perfect quantization type to represent the information in the model more efficiently\n",
        "*   bnb_4bit_compute_dtype = compute_dtype : makes sure the tools that are chosen have smaller and simpler instructions we're using for the tools we selected above\n",
        "*   bnb_4bit_use_double_quant = use_nested_quant : makes it more concise and optimized\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gl3hUXfuCQtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (you can process it here)\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")"
      ],
      "metadata": {
        "id": "GhYCZCtwCSMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   if compute_dtype == torch.float16 and use_4bit : checks if the specified compute data type is torch.float16 (half-precision floating-point) and if 4-bit quantization is being used.\n",
        "*   major, _ = torch.cude.get_device_capability() : major value indicates the generation of your GPU architecture. _ is used as a placeholder for the minor version number. provides CUDA-related functionality, allowing PyTorch to interact with NVIDIA GPUs. If CUDA is working, the function returns a tuple of two numbers: (major, minor). The purpose of retrieving the CUDA compute capability in this code is to check if the GPU supports the bfloat16 data type.\n",
        "*   if major >= 8 : GPUs with a compute capability of 8.0 or higher have native support for bfloat16, which can accelerate training by using this lower-precision data type\n",
        "*   print stmts in the if : if your GPU supports bfloat16, this code will suggest enabling it (bf16=True) for faster training.\n",
        "*   model = AutoModelForCausalLM.from_pretrained(...) : loads the pre-trained Llama 2 model from Hugging Face like the version of model we mentioned in above code\n",
        "*   model_name : the model of Llama 2 that we defined above\n",
        "*   quantization_config = bnb_config : Passes the quantization configuration (bnb_config) to the model loading function. This configuration specifies the settings for quantization, such as using 4-bit precision and the compute data type.\n",
        "*   device_map = \"auto\": Specifies the device map for distributing the model across multiple devices (e.g., GPUs). automatically optimizes the placement for you.\n",
        "*   model.config.use_cache = False : disables the attention cache, which can sometimes cause issues during fine-tuning, especially with QLoRA.\n",
        "*   model.config.pretraining_tp = 1 : Sets the tensor parallelism attribute of the model configuration to 1. This attribute is specific to the Llama 2 model and used for controlling the level of parallelism during fine-tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "iWBCVYdjgqgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "JlOVdjWVgqDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   tokenizer = AutoTokenizer.from_pretrained(...) : loads the tokenizer for the LLaMA model using the AutoTokenizer class from the Hugging Face Transformers library. from_pretrained() method is used to load the tokenizer configuration from a pretrained model specified by model_name\n",
        "*   trust_remote_code = True : set to allow loading code from remote sources, which is necessary for tokenizers.\n",
        "*   tokenizer.pad_token = tokenizer.eos_token : sets the padding token of the tokenizer to be the same as the end-of-sequence (EOS) token.Padding tokens are used to pad sequences to a fixed length during batch processing.\n",
        "By setting the padding token to the EOS token, the model will treat padded tokens as end-of-sequence markers.\n",
        "*   tokenizer.padding_side = \"right\" : sets the padding side of the tokenizer to the right side of the sequences. if its set to default i.e. on the left then it can cause issues w/ fp16. so setting to right avoids potential overflow issues during training\n",
        "*   peft_config = LoraConfig(...) : loads configurations for LoRA\n",
        "*   lora_alpha = lora_alpha : sets value to the one we defined above\n",
        "*   lora_dropout = lora_droupout : setting the value to the one we defined in above code\n",
        "*   r = lora_r : sets value as defined above\n",
        "*   bias = \"none\" : no biases are used\n",
        "*   task_type = \"CAUSAL_LM\" :  type of task the model is being fine-tuned for i.e. set to \"CAUSAL_LM\", indicating a causal language modeling task where the model predicts the next token based on the previous tokens.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vCFaArVvwXV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "fAXeI7CXwX2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   training_arguments = TrainingArguments(...) : creates class i.e. part of Transformers lib and used to define various parameters and configurations for training process.\n",
        "*   output_dir = output_dir : it specifies the dir where trained model and other files are saved.\n",
        "*   num_train_epochs = num_train_epochs : sets number of training epochs i.e. how many times the dataset will be passed through the model\n",
        "*   per_device_train_batch_size = per_device_train_batch_size : batch size is number of samples that will be passed through the model at once during training. per_device is for specific device that you are using\n",
        "*   gradient_accumulation_steps = gradient_accumulation_steps : Gradient accumulation is a technique used to effectively increase the batch size without running into memory limitations. It accumulates the gradients from multiple smaller batches before updating the model's parameters.\n",
        "*   optim = optim : this parameter specifies the optimization algorithm to be used during training i.e. AdamW\n",
        "*   save_steps = save_steps : requency at which to save the model's checkpoints during training\n",
        "*   logging_steps = logging_steps : sets the frequency at which training logs will be generated, measured in number of steps.\n",
        "*   learning_rate = learning_rate : controls how quickly the model learns from the data.\n",
        "*   weight_decay = weight_decay : regularization technique to prevent overfitting by adding a penalty term to the loss function.\n",
        "*   fp16 = fp16, bf16 = bf16 : enable the use of mixed precision training, which can improve training speed and efficiency by using lower-precision data types\n",
        "*   max_grad_norm = max_grad_norm : sets the maximum allowed gradient norm, which is used to prevent the gradients from becoming too large during training.\n",
        "*   max_steps = max_steps : sets the maximum number of training steps, which can be used as an alternative to specifying the number of training epochs\n",
        "*   warmup_ratio = warmup_ratio : sets the ratio of the total training steps used for a linear warmup of the learning rate at the beginning of training\n",
        "*   group_by_length = group_by_lenth : boolean indicating whether to group samples by their length during training  which can improve training efficiency and performance\n",
        "*   lr_scheduler_type = lr_scheduler_type :  which can improve training efficiency and performance i.e. cosine\n",
        "*   report_to = \"tensorboard\" : specifies that the training progress and metrics should be reported to TensorBoard, a popular tool for visualizing and monitoring machine learning experiments\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MbHNEKE63PgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ],
      "metadata": {
        "id": "liRLlKE33QB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   trainer = SFTTrainer(..) : instance of the SFTTrainer class\n",
        "*   model : the pre-trained model that we want to fine tune\n",
        "*   train_dataset = dataset : the dataset that we had declared above\n",
        "*   peft_config = peft_config : Parameter-Efficient Fine-Tuning (PEFT). PEFT is a technique that allows fine-tuning of LLMs with limited computational resources by only fine-tuning a subset of the model's parameters\n",
        "*   dataset_text_field = \"text\" : name of the field in the dataset that contains the text data. In this case, it is set to \"text\"\n",
        "*   max_seq_length = max_seq_length :  maximum sequence length for the input data. It determines the maximum number of tokens that the model will process at a time.\n",
        "*   tokenizer = tokenizer : tokenizer object used to tokenize the input text data into a format that the model can understand\n",
        "*   args = training_arguments : Additional training arguments specified in the training_arguments variable. These arguments may include hyperparameters, optimization settings, etc.\n",
        "*   packing = packing : packing sequences for efficient training. It is likely used to optimize memory usage during training\n",
        "*   trainer.train() : nitiates the training process using the train() method of the SFTTrainer instance. During training, the model will iterate over the provided training dataset, update its parameters based on the specified training arguments, and aim to minimize the training loss. The fine-tuning process will continue for the specified number of epochs\n",
        "*   trainer.model.save_pretrained(new_model) : saves the fine-tuned model to a specified directory new_model. The save_pretrained() method is commonly used in Hugging Face's Transformers library to save the model's weights and configuration files in a format that can be easily loaded later for inference or further fine-tuning. new_model variable should contain the path or directory where you want to save the fine-tuned model. The saved model can then be loaded using the from_pretrained() method from the same library.\n",
        "\n"
      ],
      "metadata": {
        "id": "DCHdBHBPOyeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "p9BBgJBnOzFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   logging.set_verbosity(logging.CRITICAL) : used to display messages and warnings during program execution. it tells the logging module to only display critical errors and suppress all other warnings and messages\n",
        "*   prompt = \"What is large language model?\" : defines a variable called prompt and assigns it a string value, which represents the prompt or question that will be used as input for the text generation task\n",
        "*   pipe = pipeline(..) : from the transformers library that creates a text generation pipeline\n",
        "*   task = \"text-generation\" : Specifies that you want to use the pipeline for text generation.\n",
        "*   model = model : set to the variable model, which should contain the loaded fine-tuned Llama 2 model.\n",
        "*   tokenizer = tokenizer : set to the variable tokenizer, that contains the tokenizer associated with the fine-tuned Llama 2 model.\n",
        "*   max_length = 200 : specifying the maximum length of the generated text in terms of the number of tokens.\n",
        "*   result = pipe(f\"< s >[INST] {prompt} [/INST]\") : invokes the text generation pipeline (pipe) with a formatted input string. input string is constructed using an f-string, which allows embedding the prompt variable within the string. < s > represents the start of the sequence, [INST] and [/INST] are special tokens indicating the instruction or prompt, and {prompt} is replaced with the actual prompt value. then these values are saved in result variable\n",
        "*   print(result[0]['generated_text']) : prints the generated text obtained from the text generation pipeline. result is expected to be a list containing a dictionary with the generated text.result[0] accesses the first (and likely only) dictionary in the list.['generated_text'] retrieves the value associated with the key 'generated_text' from the dictionary, which represents the generated text.\n",
        "The generated text is then printed to the console.\n",
        "\n"
      ],
      "metadata": {
        "id": "G-Xwnd4qw66K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is a large language model?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "yYpPd3kRw7M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   del model : it deletes the model object i.e. the instance of Llama 2. we're removing the reference to the model's weights, buffers, and other associated data structures, which are stored in memory. This helps to free up memory, as the model is no longer occupying space.\n",
        "*   del pipe : it deletes pipe object from memory. A pipeline is a sequence of processing steps that can include tokenization, encoding, and decoding. By deleting the pipeline, we're removing any remaining references to the model's internal state, which can also help to free up memory\n",
        "*   del trainer : deletes the trainer object, holds references to the model, optimizer, and other training-related data structures, which are now deleted\n",
        "*   import gc : imports garbage collector i.e. responsible for automatically freeing up memory that is no longer being used by the program\n",
        "*   gc.collect() : trigger the garbage collection process to reclaim memory that is no longer needed. eason for calling gc.collect() twice is to ensure that any objects that were freed up in the first call and are no longer referenced by other objects are also collected and their memory is freed.\n"
      ],
      "metadata": {
        "id": "a9zjU7qA7y-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "tNjdX9CH7zlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   base_model = AutoModelForCausalLM.from_pretrained(..) : loads a pre-trained LLaMA 2 model using the AutoModelForCausalLM class from the transformers library\n",
        "*   model_name = name of the pre-trained model to load\n",
        "*   low_cpu_mem_usage = True : this flag tells the model to use less CPU memory, which can be helpful when working with large models\n",
        "*   return_dict = True : this flag indicates that the model should return a dictionary containing the output instead of a tuple.\n",
        "*   torch_dtype = torch.float16 : this sets the data type of the model's weights and activations to float16 (FP16), which can reduce memory usage and improve performance on supported hardware.\n",
        "*   device_map = device_map : this argument specifies a device map, which defines how the model's weights and buffers are distributed across multiple devices\n",
        "*   model = PeftModel.from_pretrained(base_model, new_model) : creates a new PeftModel instance, which is a special type of model that allows for efficient fine-tuning of large language models using LoRA. base_model is pre-trained LLaMA 2 model loaded in the previous step. new_model is custom model or a configuration object that defines the LoRA weights and other modifications to the pre-trained model\n",
        "*   model = model.merge_and_unload() : it merges the LoRA weights with the pre-trained model weights. Unloads the original pre-trained model weights to save memory\n",
        "*   tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True) : it loads a pre-trained tokenizer using the AutoTokenizer class from the transformers library. model_name is name of the pre-trained model to load. trust.. flag indicates that the tokenizer should trust remote code and execute it locally.\n",
        "*   tokenizer.pad_token = tokenizer.eos_token : sets the pad_token attribute of the tokenizer to the eos_token (end-of-sequence token). This is done to ensure that the tokenizer uses the same token for padding and end-of-sequence marking.\n",
        "*   tokenizer.padding_side = \"right\" : sets the padding_side attribute of the tokenizer to \"right\", indicating that padding should be applied to the right side of the input sequences.\n",
        "\n"
      ],
      "metadata": {
        "id": "c9lQmCdkBOB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "jsq00VpOBOoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   !huggingface-cli login : using the Hugging Face CLI (Command-Line Interface) to log in to your Hugging Face account. By running this command, you are authenticating yourself with the Hugging Face platform, which is necessary for the subsequent actions you'll be performing.\n",
        "*   model.push_to_hub(new_model, use_temp_dir = False) : method provided by the Hugging Face library that allows you to upload your trained model to the Hugging Face Hub, which is a centralized repository for sharing and accessing machine learning models. new_model is the variable or object representing your fine-tuned LLaMA 2 model that you want to upload. use_temp_dir=False is a parameter that specifies whether to use a temporary directory to store the model files during the upload process. Setting it to False means that the model files will be uploaded directly from their original location, without creating a temporary directory.\n",
        "*   tokenizer.push_to_hub(new_model, use_temp_dir = False) : pushing the tokenizer associated with your fine-tuned LLaMA 2 model to the Hugging Face Hub. tokenizer.push_to_hub() is a method provided by the Hugging Face library that allows you to upload the tokenizer, which is a crucial component for preprocessing text data for your model. new_model is the same variable or object representing your fine-tuned LLaMA 2 model. use_temp_dir=False is the same parameter as before, which specifies that the tokenizer files will be uploaded directly from their original location.\n"
      ],
      "metadata": {
        "id": "TjtoUq8qHqzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "\n",
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "4KfekFz4HrLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}